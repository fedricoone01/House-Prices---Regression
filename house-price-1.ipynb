{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime\n\n#\n\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\n#\n\nfrom typing import Dict, List, Tuple, Sequence\n\n\nimport numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-06T15:03:08.071220Z","iopub.execute_input":"2022-09-06T15:03:08.071829Z","iopub.status.idle":"2022-09-06T15:03:08.086281Z","shell.execute_reply.started":"2022-09-06T15:03:08.071765Z","shell.execute_reply":"2022-09-06T15:03:08.084583Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_columns = 250\npd.options.display.max_rows = 250\n\ntrain_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.089019Z","iopub.execute_input":"2022-09-06T15:03:08.090381Z","iopub.status.idle":"2022-09-06T15:03:08.210673Z","shell.execute_reply.started":"2022-09-06T15:03:08.090328Z","shell.execute_reply":"2022-09-06T15:03:08.209377Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"train_data.describe().round(2)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.213084Z","iopub.execute_input":"2022-09-06T15:03:08.214487Z","iopub.status.idle":"2022-09-06T15:03:08.372181Z","shell.execute_reply.started":"2022-09-06T15:03:08.214434Z","shell.execute_reply":"2022-09-06T15:03:08.370858Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ntrain_data.size","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.373715Z","iopub.execute_input":"2022-09-06T15:03:08.374122Z","iopub.status.idle":"2022-09-06T15:03:08.382864Z","shell.execute_reply.started":"2022-09-06T15:03:08.374088Z","shell.execute_reply":"2022-09-06T15:03:08.381123Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.385789Z","iopub.execute_input":"2022-09-06T15:03:08.386679Z","iopub.status.idle":"2022-09-06T15:03:08.394653Z","shell.execute_reply.started":"2022-09-06T15:03:08.386640Z","shell.execute_reply":"2022-09-06T15:03:08.392967Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"train_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.396489Z","iopub.execute_input":"2022-09-06T15:03:08.397087Z","iopub.status.idle":"2022-09-06T15:03:08.414638Z","shell.execute_reply.started":"2022-09-06T15:03:08.397047Z","shell.execute_reply":"2022-09-06T15:03:08.413507Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"train_data['YrSold'].unique()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.416558Z","iopub.execute_input":"2022-09-06T15:03:08.417274Z","iopub.status.idle":"2022-09-06T15:03:08.424363Z","shell.execute_reply.started":"2022-09-06T15:03:08.417232Z","shell.execute_reply":"2022-09-06T15:03:08.423483Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#Eliminamos las variables sin sentido. En este caso #Id\"\n#train_data.drop(\"Id\", axis=1, inplace=True)\n#test_data.drop('Id', axis=1, inplace=True)\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.425789Z","iopub.execute_input":"2022-09-06T15:03:08.426195Z","iopub.status.idle":"2022-09-06T15:03:08.489223Z","shell.execute_reply.started":"2022-09-06T15:03:08.426160Z","shell.execute_reply":"2022-09-06T15:03:08.487798Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"#Hacemos un analisis de correlación\n\n#OBSERVACION: tanto en el grafico de correlación como en algunas funciones de aca arriba, -\n# solo aparecen las variables \"numéricas\" Y NO LAS CATEGÓRICAS. \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nplt.style.use('fivethirtyeight')\n\n\nsns.set(font_scale=1.1)\ncorrelation_train = train_data.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.491070Z","iopub.execute_input":"2022-09-06T15:03:08.491505Z","iopub.status.idle":"2022-09-06T15:03:11.868015Z","shell.execute_reply.started":"2022-09-06T15:03:08.491464Z","shell.execute_reply":"2022-09-06T15:03:11.866558Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"#Analizamos las \"variables NUMERICAS\"\n\n#Una de las mejores maneras de ver cómo afectan los precios de venta son los \"diagramas de dispersión\". \n#También estamos trazando líneas de regresión polinómicas para ver la tendencia general.-\n# de esta manera podemos entender los valores numéricos y su importancia en el precio de venta,\n#también es muy útil para --> detectar valores atípicos <--.\n\n\ndef srt_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n\nsrt_reg('SalePrice', train_data)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:11.869912Z","iopub.execute_input":"2022-09-06T15:03:11.870756Z","iopub.status.idle":"2022-09-06T15:03:53.428723Z","shell.execute_reply.started":"2022-09-06T15:03:11.870701Z","shell.execute_reply":"2022-09-06T15:03:53.426942Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Eliminar valores atípicos (después de detectarlos a simple vista).\n\n\ntrain_data = train_data.drop(train_data[(train_data['OverallQual'] < 5) & (train_data['SalePrice'] > 200000)].index)\n\ntrain_data = train_data.drop(train_data[(train_data['GrLivArea'] > 4000)& (train_data['SalePrice'] < 200000)].index)\n\ntrain_data = train_data.drop(train_data[(train_data['GarageArea'] > 1200)& (train_data['SalePrice'] < 200000)].index)\n\ntrain_data = train_data.drop(train_data[(train_data['TotalBsmtSF'] > 3000)& (train_data['SalePrice'] > 320000)].index)\n\ntrain_data = train_data.drop(train_data[(train_data['1stFlrSF'] < 3000)& (train_data['SalePrice'] > 600000)].index)\n\ntrain_data = train_data.drop(train_data[(train_data['1stFlrSF'] > 3000)& (train_data['SalePrice'] < 200000)].index)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:53.433155Z","iopub.execute_input":"2022-09-06T15:03:53.433635Z","iopub.status.idle":"2022-09-06T15:03:53.467581Z","shell.execute_reply.started":"2022-09-06T15:03:53.433592Z","shell.execute_reply":"2022-09-06T15:03:53.466302Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"#Analizamos ahoras las \"variables CATEGORICAS\" en relación al \"precio de venta\" de las casas.\n\ndef srt_box(y, df):\n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()\n\nsrt_box('SalePrice', train_data)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:53.469333Z","iopub.execute_input":"2022-09-06T15:03:53.469793Z","iopub.status.idle":"2022-09-06T15:04:49.377109Z","shell.execute_reply.started":"2022-09-06T15:03:53.469734Z","shell.execute_reply":"2022-09-06T15:04:49.375873Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# Extraemos la variable objetivo del \"dataset de entrenamiento\"\n\ny = train_data['SalePrice'].reset_index(drop=True)\n#train_data = train_data.drop(['SalePrice'], axis=1)\n\n\n# Unimos los datos, para no tener que hacer 2 veces cada operacion de transformación\n\ndataset = pd.concat([train_data, test_data]).reset_index(drop=True)\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:49.379000Z","iopub.execute_input":"2022-09-06T15:04:49.379355Z","iopub.status.idle":"2022-09-06T15:04:49.407170Z","shell.execute_reply.started":"2022-09-06T15:04:49.379323Z","shell.execute_reply":"2022-09-06T15:04:49.405630Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"#Ahora vamos a ver los datos faltantes\n\n#Lo que podemos hacer es verlos en seco(como esta mas arriba del codigo) o verlos en graficos para asi visualizarlos mejor\n\ndef missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) / len(df) *\n                     100)[(df.isnull().sum().sort_values(ascending=False) / len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n\nmissing = missing_percentage(dataset)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:49.408744Z","iopub.execute_input":"2022-09-06T15:04:49.409125Z","iopub.status.idle":"2022-09-06T15:04:50.035422Z","shell.execute_reply.started":"2022-09-06T15:04:49.409092Z","shell.execute_reply":"2022-09-06T15:04:50.034100Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"#Pasamos a corregir este problema de los datos NaN y null\n\n# Lista de NaN, incluidas las columnas donde NaN significa ninguno.\nnone_cols = ['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n             'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n             'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n\n# Lista de NaN, incluidas las columnas donde NaN significa 0.\nzero_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n             'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea']\n\n# Lista de NaN, incluidas las columnas donde NaN realmente falta, se reemplazará con el modo.\nfreq_cols = ['Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n             'SaleType', 'Utilities']\n\n\n# Llenando la lista de columnas arriba:\n\nfor col in zero_cols:\n    dataset[col].replace(np.nan, 0, inplace=True)\n\nfor col in none_cols:\n    dataset[col].replace(np.nan, 'None', inplace=True)\n\nfor col in freq_cols:\n    dataset[col].replace(np.nan, dataset[col].mode()[0], inplace=True)\n    \n#La caracteristica de \"MSZoning\" y\"Lot Frontage\" son un poco complicada -\n# se lo rellenamos con el tipo más común de cada categoría - \n# no es perfecto, pero al menos disminuimos un poco la aleatoriedad.\n\ndataset['MSZoning'] = dataset.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))\n\n# Filling LotFrontage according to Neighborhood\ndataset['LotFrontage'] = dataset.groupby(\n    ['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.036861Z","iopub.execute_input":"2022-09-06T15:04:50.037242Z","iopub.status.idle":"2022-09-06T15:04:50.094072Z","shell.execute_reply.started":"2022-09-06T15:04:50.037205Z","shell.execute_reply":"2022-09-06T15:04:50.092645Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# Convertimos tipos de variables que son \"números\", pero que deberian ser tratados como \"strings\"\n\ndataset['MSSubClass'] = dataset['MSSubClass'].astype(str)\ndataset['YrSold'] = dataset['YrSold'].astype(str)\ndataset['MoSold'] = dataset['MoSold'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.095541Z","iopub.execute_input":"2022-09-06T15:04:50.095966Z","iopub.status.idle":"2022-09-06T15:04:50.111333Z","shell.execute_reply.started":"2022-09-06T15:04:50.095928Z","shell.execute_reply":"2022-09-06T15:04:50.109926Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"#Los valores realmente raros y que parecen no agregar mucho en general,-\n# (aparecen menos de 10 veces en nuestras observaciones) entran en el grupo \"Otros\". - \n# Es reducción de la dimensionalidad\n\nothers = ['Condition1', 'Condition2', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n          'Heating', 'Electrical', 'Functional', 'SaleType']\n\nfor col in others:\n    mask = dataset[col].isin(dataset[col].value_counts()[dataset[col].value_counts() < 10].index)\n    dataset[col][mask] = 'Other'\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.113254Z","iopub.execute_input":"2022-09-06T15:04:50.113634Z","iopub.status.idle":"2022-09-06T15:04:50.157634Z","shell.execute_reply.started":"2022-09-06T15:04:50.113600Z","shell.execute_reply":"2022-09-06T15:04:50.156365Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"#Convertimos variables CATEGORICAS en NUMERICAS\n\n#Lo que se hace es agarrar cada Feature y a todos sus posibles valores -> asigarle un número. \n#Donde 1, 2 lo es mas, y asi sucesivamente..\n\nneigh_map = {\n    'MeadowV': 1,\n    'IDOTRR': 1,\n    'BrDale': 1,\n    'BrkSide': 2,\n    'OldTown': 2,\n    'Edwards': 2,\n    'Sawyer': 3,\n    'Blueste': 3,\n    'SWISU': 3,\n    'NPkVill': 3,\n    'NAmes': 3,\n    'Mitchel': 4,\n    'SawyerW': 5,\n    'NWAmes': 5,\n    'Gilbert': 5,\n    'Blmngtn': 5,\n    'CollgCr': 5,\n    'ClearCr': 6,\n    'Crawfor': 6,\n    'Veenker': 7,\n    'Somerst': 7,\n    'Timber': 8,\n    'StoneBr': 9,\n    'NridgHt': 10,\n    'NoRidge': 10\n}\n\ndataset['Neighborhood'] = dataset['Neighborhood'].map(neigh_map).astype('Int64')\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset['ExterQual'] = dataset['ExterQual'].map(ext_map).astype('Int64')\ndataset['ExterCond'] = dataset['ExterCond'].map(ext_map).astype('Int64')\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset['BsmtQual'] = dataset['BsmtQual'].map(bsm_map).astype('Int64')\ndataset['BsmtCond'] = dataset['BsmtCond'].map(bsm_map).astype('Int64')\nbsmf_map = {\n    'None': 0,\n    'Unf': 1,\n    'LwQ': 2,\n    'Rec': 3,\n    'BLQ': 4,\n    'ALQ': 5,\n    'GLQ': 6\n}\n\ndataset['BsmtFinType1'] = dataset['BsmtFinType1'].map(bsmf_map).astype('Int64')\ndataset['BsmtFinType2'] = dataset['BsmtFinType2'].map(bsmf_map).astype('Int64')\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset['HeatingQC'] = dataset['HeatingQC'].map(heat_map).astype('Int64')\ndataset['KitchenQual'] = dataset['KitchenQual'].map(heat_map).astype('Int64')\ndataset['FireplaceQu'] = dataset['FireplaceQu'].map(bsm_map).astype('Int64')\ndataset['GarageCond'] = dataset['GarageCond'].map(bsm_map).astype('Int64')\ndataset['GarageQual'] = dataset['GarageQual'].map(bsm_map).astype('Int64')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.159366Z","iopub.execute_input":"2022-09-06T15:04:50.159857Z","iopub.status.idle":"2022-09-06T15:04:50.208516Z","shell.execute_reply.started":"2022-09-06T15:04:50.159810Z","shell.execute_reply":"2022-09-06T15:04:50.207171Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# Creando nuevas características basadas en observaciones previas\n#EJ: se creó una nueva característica que agrupa todos los tiipos de Baños en uno solo.\n\ndataset['TotalSF'] = (dataset['BsmtFinSF1'] + dataset['BsmtFinSF2'] +\n                       dataset['1stFlrSF'] + dataset['2ndFlrSF'])\ndataset['TotalBathrooms'] = (dataset['FullBath'] +\n                              (0.5 * dataset['HalfBath']) +\n                              dataset['BsmtFullBath'] +\n                              (0.5 * dataset['BsmtHalfBath']))\n\ndataset['TotalPorchSF'] = (dataset['OpenPorchSF'] + dataset['3SsnPorch'] +\n                            dataset['EnclosedPorch'] +\n                            dataset['ScreenPorch'] + dataset['WoodDeckSF'])\n\ndataset['YearBlRm'] = (dataset['YearBuilt'] + dataset['YearRemodAdd'])\n\n# Fusión de calidad y condiciones\n\ndataset['TotalExtQual'] = (dataset['ExterQual'] + dataset['ExterCond'])\ndataset['TotalBsmQual'] = (dataset['BsmtQual'] + dataset['BsmtCond'] +\n                            dataset['BsmtFinType1'] +\n                            dataset['BsmtFinType2'])\ndataset['TotalGrgQual'] = (dataset['GarageQual'] + dataset['GarageCond'])\ndataset['TotalQual'] = dataset['OverallQual'] + dataset['TotalExtQual'] + dataset['TotalBsmQual'] + dataset[\n        'TotalGrgQual'] + dataset['KitchenQual'] + dataset['HeatingQC']\n\n# Creación de nuevas funciones mediante el uso de nuevos indicadores de calidad\n\ndataset['QualGr'] = dataset['TotalQual'] * dataset['GrLivArea']\ndataset['QualBsm'] = dataset['TotalBsmQual'] * (dataset['BsmtFinSF1'] +\n                                                  dataset['BsmtFinSF2'])\ndataset['QualPorch'] = dataset['TotalExtQual'] * dataset['TotalPorchSF']\ndataset['QualExt'] = dataset['TotalExtQual'] * dataset['MasVnrArea']\ndataset['QualGrg'] = dataset['TotalGrgQual'] * dataset['GarageArea']\ndataset['QlLivArea'] = (dataset['GrLivArea'] -\n                         dataset['LowQualFinSF']) * (dataset['TotalQual'])\ndataset['QualSFNg'] = dataset['QualGr'] * dataset['Neighborhood']\n\n\n# Creando algunas características simples\n\ndataset['HasPool'] = dataset['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndataset['Has2ndFloor'] = dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasGarage'] = dataset['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasBsmt'] = dataset['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasFireplace'] = dataset['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasPorch'] = dataset['QualPorch'].apply(lambda x: 1 if x > 0 else 0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.210706Z","iopub.execute_input":"2022-09-06T15:04:50.211935Z","iopub.status.idle":"2022-09-06T15:04:50.259694Z","shell.execute_reply.started":"2022-09-06T15:04:50.211880Z","shell.execute_reply":"2022-09-06T15:04:50.258581Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"##Transformando los datos\n#Algunos de los valores continuos no se distribuyen de manera uniforme y no se ajustan a la distribución normal- \n# podemos solucionarlos mediante el uso de enfoques de transformación de pareja. Vamos a usar boxcox\n\npossible_skewed = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n                    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n                    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n                    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal']\n\n# Encontrar la asimetría de las características numéricas.\n\nskew_features = np.abs(dataset[possible_skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtrado de características sesgadas.\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Toma de índices de alto sesgo.\n\nskew_index = high_skew.index\n\n# Aplicar la transformación boxcox para corregir la asimetría.\n\nfor i in skew_index:\n    dataset[i] = boxcox1p(dataset[i], boxcox_normmax(dataset[i] + 1))\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.261244Z","iopub.execute_input":"2022-09-06T15:04:50.262426Z","iopub.status.idle":"2022-09-06T15:04:50.420568Z","shell.execute_reply.started":"2022-09-06T15:04:50.262381Z","shell.execute_reply":"2022-09-06T15:04:50.419108Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"#Codificación de etiquetas de las variables categóricas\n\ndataset = pd.get_dummies(data=dataset)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.422678Z","iopub.execute_input":"2022-09-06T15:04:50.423352Z","iopub.status.idle":"2022-09-06T15:04:50.475339Z","shell.execute_reply.started":"2022-09-06T15:04:50.423312Z","shell.execute_reply":"2022-09-06T15:04:50.473994Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"#Chequeamos, antes de empezar con el modelado\n\ndataset.drop(columns='SalePrice', inplace=True)\nprint(f'Number of missing values: {dataset.isna().sum().sum()}')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.476884Z","iopub.execute_input":"2022-09-06T15:04:50.477272Z","iopub.status.idle":"2022-09-06T15:04:50.497851Z","shell.execute_reply.started":"2022-09-06T15:04:50.477235Z","shell.execute_reply":"2022-09-06T15:04:50.496593Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"#----Entrenamiento----\n\n# Separamos el dataset\ntrain = dataset.iloc[:len(y), :]\ntest = dataset.iloc[len(train):, :]\n\n\nprint(len(test))\nprint(len(train))","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.499357Z","iopub.execute_input":"2022-09-06T15:04:50.499908Z","iopub.status.idle":"2022-09-06T15:04:50.508835Z","shell.execute_reply.started":"2022-09-06T15:04:50.499864Z","shell.execute_reply":"2022-09-06T15:04:50.507477Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"X = train\nX_test = test\ny = np.log1p(y)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.510447Z","iopub.execute_input":"2022-09-06T15:04:50.511322Z","iopub.status.idle":"2022-09-06T15:04:50.520836Z","shell.execute_reply.started":"2022-09-06T15:04:50.511109Z","shell.execute_reply":"2022-09-06T15:04:50.519508Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# Cargando paquetes necesarios para el modelado:\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.522949Z","iopub.execute_input":"2022-09-06T15:04:50.523837Z","iopub.status.idle":"2022-09-06T15:04:50.536336Z","shell.execute_reply.started":"2022-09-06T15:04:50.523760Z","shell.execute_reply":"2022-09-06T15:04:50.535140Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# Configuración de kfold para uso futuro.\nkf = KFold(10, random_state=42, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.538031Z","iopub.execute_input":"2022-09-06T15:04:50.538810Z","iopub.status.idle":"2022-09-06T15:04:50.547555Z","shell.execute_reply.started":"2022-09-06T15:04:50.538734Z","shell.execute_reply":"2022-09-06T15:04:50.546061Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"#Modelado\n\n\nalphas_alt = [30.5, 20.6, 20.7, 20.8, 20.9, 20, 20.1, 20.2, 20.3, 20.4, 20.5]\nalphas2 = [0.01]\ne_alphas = [0.01]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv:\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,))\n\n# lasso_cv:\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e3, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv:\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e3,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr:\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\n# gradientboosting:\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n# xgboost:\nxgboost = XGBRegressor(\n    learning_rate =0.0139,\n    n_estimators =4500,\n    max_depth =4,\n    min_child_weight =0,\n    subsample =0.7968,\n    colsample_bytree =0.4064,\n    nthread =-1,\n    scale_pos_weight =2,\n    seed=42,\n    enable_categorical=True)\n\n\n# histgradientboost:\nhgrd= HistGradientBoostingRegressor(loss= 'least_squares',\n    max_depth = 2,\n    min_samples_leaf = 40,\n    max_leaf_nodes = 29,\n    learning_rate = 0.15,\n    max_iter = 500,\n    random_state=42)\n\n#tweedie regresson:\ntweed = make_pipeline(RobustScaler(),TweedieRegressor(alpha=0.005))\n\n\n# stacking regressor:\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm,hgrd, tweed),\n                                            meta_regressor=xgboost,\n                                            use_features_in_secondary=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.550149Z","iopub.execute_input":"2022-09-06T15:04:50.550598Z","iopub.status.idle":"2022-09-06T15:04:50.568048Z","shell.execute_reply.started":"2022-09-06T15:04:50.550562Z","shell.execute_reply":"2022-09-06T15:04:50.566748Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# Validación cruzada\n\ndef model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n        print(\"--------\")\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.569916Z","iopub.execute_input":"2022-09-06T15:04:50.570797Z","iopub.status.idle":"2022-09-06T15:04:50.585410Z","shell.execute_reply.started":"2022-09-06T15:04:50.570725Z","shell.execute_reply":"2022-09-06T15:04:50.584295Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Setting list of estimators and labels for them.\n\nestimators = [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, hgrd, tweed]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor'\n]","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.592475Z","iopub.execute_input":"2022-09-06T15:04:50.593191Z","iopub.status.idle":"2022-09-06T15:04:50.601933Z","shell.execute_reply.started":"2022-09-06T15:04:50.593128Z","shell.execute_reply":"2022-09-06T15:04:50.600481Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Executing cross validation.\n\nraw_models = model_check(X, y, estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:04:50.603795Z","iopub.execute_input":"2022-09-06T15:04:50.604337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the models on train data:\n\n#APILAR: El apilamiento es una técnica de aprendizaje de conjunto para combinar múltiples modelos de regresión \n#a través de un metarregresor\n\n#Apilar y mezclar\n#Aquí ajustamos cada estimador que tenemos en los datos del tren y luego \n#los combinamos asignando pesos a cada modelo y sumamos los resultados.\n#Los pesos son bastante subjetivos y estoy bastante seguro de que puedes encontrar algo que \n#funcione mejor que esto si juegas con él...\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\n#print(datetime.now(), 'StackingCVRegressor')\n#stack_gen_model = stack_gen.fit(X.values, y.values)\n\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y)\n\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\n#print(datetime.now(), 'XGboost')\n#xgb_model_full_data = xgboost.fit(X, y)\n\n#print(datetime.now(), 'Lightgbm')\n#lgb_model_full_data = lightgbm.fit(X, y)\n\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y)\n\nprint(datetime.now(), 'Tweed')\ntweed_full_data = tweed.fit(X, y)\n\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtengo las Predicciones y Evaluo la precisión de cada modelo (en 3 formas diferentes)\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\n#ACLARACION: en este caso estan mal tomados los datos de entrenamiento aca abajo. Hay un error\n#No lo pude corregir por diferencia entre una pequeña longitud entre X y X_test. Dejando eso de lado, el resto esta bien\n\n\n#y1 = elastic_model_full_data.predict(X_test)\nprint(\"\\nelastic_model_full_data\")\ny1 = elastic_model_full_data.predict(X)\nprint(\"MAE\",mean_absolute_error(y1, y))\nprint(\"MSE\",mean_squared_error(y1, y))\nprint(\"RMSE\",np.sqrt(mean_squared_error(y1, y)))\n\nprint(\"\\nlasso_model_full_data\")\ny2 = lasso_model_full_data.predict(X)\nprint(\"MAE\",mean_absolute_error(y2, y))\nprint(\"MSE\",mean_squared_error(y2, y))\nprint(\"RMSE\",np.sqrt(mean_squared_error(y2, y)))\n\nprint(\"\\nridge_model_full_data\")\ny3 = ridge_model_full_data.predict(X)\nprint(\"MAE\",mean_absolute_error(y3, y))\nprint(\"MSE\",mean_squared_error(y3, y))\nprint(\"RMSE\",np.sqrt(mean_squared_error(y3, y)))\n\nprint(\"\\nsvr_model_full_data\")\ny4 = svr_model_full_data.predict(X)\nprint(\"MAE\",mean_absolute_error(y4, y))\nprint(\"MSE\",mean_squared_error(y4, y))\nprint(\"RMSE\",np.sqrt(mean_squared_error(y4, y)))\n\nprint(\"\\ngbr_model_full_data\")\ny5 = gbr_model_full_data.predict(X)\nprint(\"MAE\",mean_absolute_error(y5, y))\nprint(\"MSE\",mean_squared_error(y5, y))\nprint(\"RMSE\",np.sqrt(mean_squared_error(y5, y)))\n\nprint(\"\\nhist_full_data\")\ny6 = hist_full_data.predict(X)\nprint(\"MAE\",mean_absolute_error(y6, y))\nprint(\"MSE\",mean_squared_error(y6, y))\nprint(\"RMSE\",np.sqrt(mean_squared_error(y6, y)))\n\nprint(\"\\ntweed_full_data\")\ny7 = tweed_full_data.predict(X)\nprint(\"MAE\",mean_absolute_error(y7, y))\nprint(\"MSE\",mean_squared_error(y7, y))\nprint(\"RMSE\",np.sqrt(mean_squared_error(y7, y)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combinar modelos (dandole una ponderación a cada uno)\n\ndef blend_models_predict(X):\n    return ((0.05 * elastic_model_full_data.predict(X)) +\n            (0.05 * lasso_model_full_data.predict(X)) +\n            (0.25 * ridge_model_full_data.predict(X)) +\n            (0.25 * svr_model_full_data.predict(X)) +\n            (0.05 * gbr_model_full_data.predict(X)) +\n            (0.1 * hist_full_data.predict(X)) +\n            (0.25 * tweed_full_data.predict(X)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Nuestros modelos están ajustados, apilados y combinados para que \n#podamos predecir y enviar nuestros resultados.\n\nsubmission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\n# Predicciones de precios de venta a escala de registro de inversión y suelo para ver los precios reales nuevamente.\n\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))\n\n# Creando dataframe de envío.\n\nsubmission = submission[['Id', 'SalePrice']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Guardando como un archivo csv:\n\nsubmission.to_csv('mysubmission.csv', index=False)\n\nprint('Saving submission.',datetime.now(),)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}